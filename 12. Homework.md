## RL From Human Feedback
DAGGER (human all the time ) --- Human Sometimes -- Human (Demonstration Only)
Bradley-Terry Model 
* Models internal preferences over items to how we might compare them.
* Consider k-armed bandits: K actions b1, b2, ..., bk. No state or context.
* Assume a human makes noisy pairwise comparisons, wher ethe probability she preferecnes bi > bj
$$P(b_i \succ b_j) = \frac{\exp (r(b_i))}{\exp (r(b_i)) + \exp (r(b_j))} = p_{ij}$$
* exponential reward of (b_j)
* It's a transitive property $p_{ik}$ is determined from $p_{ij}$ and $p_{jk}$
"Reinforcement learning from feedback"
Different preference models define what is "best"
Condocert: beat all other options, you'll always choose it over others.
Copeland: If it has the highest number of pairwise numbers of everything else on average.
Borda Winner: Maximizes the expected score 
Historically k-armed or dueling k=2 bandits focused on finding a codepand winner.
how do you learn this? You want to extract the reward functions of each action. 
![[Pasted image 20250104161121.png]]
### What's Cross Entropy?

## Bandit Algorithms


## Bayesian Methods


## Advanced Methods in RL