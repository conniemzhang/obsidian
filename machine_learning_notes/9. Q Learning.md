**What is the Difference Between $Q_\theta(s,a)$ and $Q(s,a)$?**
**1. $Q(s,a)$**
- The true Q-function or table of Q-values.
- Defined for every state-action pair (s,a).
- In tabular Q-learning, $Q(s,a)$ is stored directly as a table or array, and updates are made to these entries.

**2. $Q_\theta(s,a)$**
- An **approximation** of the true Q-function.
- Instead of storing a table, we use parameters $\theta$ to represent the Q-function.
- $Q_\theta​(s,a)$ is computed using a parameterized model, which in this case is linear: $$Q_\theta(s,a)=θ^T\delta(s,a)$$
- Qθ(s,a)Q_\theta(s, a)Qθ​(s,a) depends on the structure of θ\thetaθ and how we define δ(s,a)\delta(s, a)δ(s,a).

In essence:

- Q(s,a)Q(s, a)Q(s,a): Exact Q-value stored directly in memory.
- Qθ(s,a)Q_\theta(s, a)Qθ​(s,a): Approximated Q-value computed using θ\thetaθ and features.


Greedy in the Limit of Infinite Exploration (GLIE)
- Behavior policy (policy used to act in the world) converges to greedy policy.\A simple GLIE strategy is $\epsilon$-
- Theorem: GLIE Monte-Carlo control converges to the optimal state-action value fucntion 
- One of the reasons why you think about e-greedy algorithms because you get an optimal function while acting in the world.'



DQN: Experience Replay
* To help remove correlations, store dataset D from prior experience.

Theres 1Q function in supervised learning?
### Linear Approximation
***Summary***: Value function or the Q-function is approximated as a linear combination of features. Used in cases where the state or action space is large, and computing or storing the exact value function is infeasible.

Definition
Suppose we want to approximate a value function $V(s)$ or $Q(s, a)$. In linear approximation, we represent the value function as:
$$ V_{\theta}(s;\theta) = \theta^T\delta(s) $$
$$Q_{\theta}(s,a;\theta) = \theta^T\delta(s,a)$$
where 
* $\delta(s)$ or $\delta(s,a)$ is a feature vector that captures properties of the state s (and action a).
* $\theta$ is the vector of weights for each feature.
* V is action function, and Q is the action-value function.

