#machine_learning #notes 
There's two camps of machine learning, one that thinks everything should be probabilistically modeled, and another that is more interested in nice estimates.

| Structure  | Probability     | Non Probability |
| ---------- | --------------- | --------------- |
| "Cluster"  | GMM             | K-Means         |
| "Subspace" | Factor Analysis | PCA             |

# Factor Analysis
tl;dr Reduces dimensionality by modeling data $x\in\mathbb{R}^d$ in a lower k-dimension subspace where $k<<d$. Each point $x^{(i)}$ is generated by mapping to a $z^{(i)}$ that lies in the k-dimensional affine space and adding a $\Psi$-covariance noise. 
- Based on a probabilistic model
- Requires the EM algorithm to do parameter estimation. 

# PCA  - Principal Component Analysis
Dimensionality Reduction and Visualization, an older method. 
- Tries to identify the subspace on which the data lies. 
- Only requires an eigenvector calculation.
- The direct analogue to Factor Analysis for non probability. 

Ex: Given pairs (Highway MPG, City MPG) of cars
Question: "Good MPG"
PCA is typically used when you don't know too much about the data and you want to visualize it to understand it. 
<missing image oop> 
1. Center the data  $\mu = \frac{1}{n} \sum^n_{i = 1}x^{(i)}$
2. The vector from the center is the principal variation. 
We can write $x^{(i)} = \alpha^{(i)}_1u_1 + \alpha^{(i)}_2u_2$ where alpha is the transform that is the closest point to the line. 
Also functions as a dimensionality reduction - 1000s of dimension -> 2 or 3. It finds the three with the most variation? 
Assuming there's some linear submap. 

Prepocessing
Given 
1. Center the data
2. May need to rescale (divide by the variatnce in that direction)

PCA as a Optimization Problem