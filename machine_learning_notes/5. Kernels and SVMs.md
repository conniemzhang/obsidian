#machine_learning #notes 
### Kernels
Deals with nonlinearity in your data (before deep learning took off)
$$h_\theta(x) = \theta_3x^3 + \theta_2x^2 + \theta_1x^1 + \theta_0$$ for $x \in \mathbb{R}$
You are linear in $\theta$ but non linear in $x$. For linear regression it doesn't matter if you are linear in x, just that you are linear in the parameters you are optimizing over. 
So turn it into a normal linear regressino equation $\phi : \mathbb{R} -> \mathbb{R}^4$
$$\phi(x) = [1, x, x^2, x^3]^T$$
$$h_\theta(x) = [\theta_0, \theta_1, \theta_2, \theta_3] * \phi(x) = \theta^T\phi(x)$$ $h_theta(x)$ is linear in $\theta$ and $\phi(x)$. 
And you can transform the data into this new space
$$\phi(x^{(1)}, y^{(1)}), ... , \phi(x^{(n)}, y^{(n)})$$
which is your new inputs/features
And can treat it like normal linear regression, and use Gradient Descent. 

K is a valid kernel if there exists a $\phi$ such that $K(x, z) = <\phi(x), \phi(z)>$
At test time
Given a x (test example) and $x^{(i)}$ (training set), how to compute $\theta^T\phi(x)$? $$ $$
How do you design a good kernel?
1. Design a kernel function K
2. Verify it is a valid one
	1. Either by Theorem:
		
	1. Or by constructing the $\phi(x)$
4. Run the algo - turn an algo about $\phi(x)$ into one that just uses $k$ "kernel trick"
	1. Some algorithms cannot be kernalized. 
		1. If you algorithm cannot be written in a way where its represented by its dot product. 

Example
$$K(x, a) = (<x, z> + c)^k$$
for $k = 2, \phi(x) = [c, \sqrt{2c}x_1, ... x_1^2 .... x_d^2]^T$

Gaussian Kernel:
$$ K(x,z) = \exp(-\frac{||x-z||^2}{2\sigma^2}) = <\phi(x), \phi(z)>$$
But $\phi$ has to be infinite dimensional vector (??). Very complicated phi, but we don't care about it since we just know the kernel, and the inner product can be computed efficiently.


stochastic gradient descent update rule
$$\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta({x^{(i)}}))x^{(i)}_j)$$

3ai perceptron update: 
$$\theta^{(i)} := \theta^{(i-1)} + \alpha(y^{(i)} - g(\theta^T\phi({x^{(i)}}))\phi(x^{(i)})$$

If $\theta$ is represented as
$$ h_{\theta^{(i)}}(x^{(i+1)})  = g(\sum^{i+1}_{k=1}\beta_k\phi(x^{(i)})\phi(x^k))$$
