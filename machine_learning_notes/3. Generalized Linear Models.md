#machine_learning #notes #class 
## Exponential Family Models

Goal: unify inference and learning for many models

#### Exponential Family
PDF Idea: If P has a special form, inference (hypothesis) and learning simplify. 
$$P(y ; \eta) = b(y)exp[\eta^TT(y) - a(\eta)]$$
- $\eta$ is the natural parameters (what the fuck is that)
- $y$ is the data.
- $T(y)$ is a sufficient statistic. Summarizes all the information in T(y)?
- $T(y)$ and $\eta$ are the same dimension due to the dot product in the definition (since both of them are vectors).
	- We currently simplify $T(y) = y$
- $b(y)$ is a base measure, and only depends on the data and does not depend on $\theta$
- $a(\eta)$ is called a log partition function. This does not depend on $y$!
	- Makes sure this is a probability distribution.
- $a(\eta), b(y)$ are scalars. 
- This form represents many distributions that are common.

**Ex: Bernoulli Distribution**
$$p(y ; \phi) = \phi^y(1-\phi)^{1-y}$$
can be written in the normal form which is equivalent. 
>Also phi is the canonical parameters, not the natural parameters. It requires this transform to be a natural parameter.

$$\begin{align} p(y ; \phi) & = exp(y \log(\phi) + (1-y)\log1-\phi) 
\\ & = exp(log\frac{\phi}{1-\phi}y + \log(1-\phi)) \end{align}$$
This matches the special form above, highlighting $y$ as the sufficient statistic, $\log\left(\frac{\phi}{1-\phi}\right)$ as the natural parameter, $b(y) = 1$, and $-a(\eta) = \log(1-\phi)$ as part of the log-partition function.
This last $-a(\eta) = \log(1-\phi)$ is a claim and needs to be proved that you can swap in $\eta$ for $\phi$.

 **Ex: Gaussian (w/ fixed variance $\sigma = 1$)**
$$p(y; \mu) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y-\mu)^2}{2}\right)$$
$$\begin{align} p(y | \mu) & = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{y^2 - 2y\mu + \mu^2}{2}\right) \\ 
& =\frac{1}{\sqrt2\pi}\exp^{-\frac{y^2}{2}}exp\left(\mu y - \frac{1}{2}\mu^2\right) \end{align}$$
So this shows that the Gaussian is in the exponential family. But what can we do with these forms? Why do we care?
> What the hell is a log partition?
1. Inference is "Easy" $\mathbb{E}[y ; \eta] = \frac{d}{d\eta}a{\eta}$
	* Taking a derivative is the same as taking the expectation.
2. Learning is well-defined.
	* MLE (Maximum Likelihood Estimator) wrt to $\eta$ is concave, which means that Negative Log Likelihood is convex.
Missing a hypothesis...

## Generalized Linear Models
Design Choices (the assumptions we make)
1) $y | x_h\theta \in~$ exponential family, and they model certain situations.
	- y is binary --> Bernoulli
	- y is real --> Gaussian
	- y is counts --> Poisson
	- y is positive real --> Gamma/Exponential 
	- Distribution --> Dirdicht?? 
2) $\eta = \theta^Tx$ and $\theta \in \mathbb{R}^d, x \in \mathbb{R}^d$ 
	* the value of the parameter is given by some other model weight \theta and some features \x (this is automatic, it's why it's a linear model.
3) Inference @ test time is that the output is $\mathbb{E}[y|x_j\theta]$ i.e. $h_\theta(x) =\mathbb{E}y | x_j\theta]$. 
![[Pasted image 20240528203704.png]]

#### Link Function


#### Maximum Likelihood Estimate

the exponential model is the error model.

The  liklihood method is a measure of how well a particular model fits the data. It explains how well a parmeter explains the observed data. 
A log likelihood function is used in maximum likelihood estimation 

