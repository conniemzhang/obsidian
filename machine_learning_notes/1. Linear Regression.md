#machine_learning #notes #class
## Basics and Notation
Using a single parameter that in this example maps sq ft to house price $x_1$. h the hypothesis could have been any function, but we've chosen this affine representation.
$$h(x) = \theta_0 +\theta_1x_1$$
Each feature is multiplied by a parameter $\theta$ which is also called the *bias term*. So then you can expand it to $d$ features, like bedrooms, location, lot size. These become your {x}. 
You can write this as (because of the $\theta_0$ is multiplied by an identity $x_0 = 1$). j is the index of the feature/parameter pair. 
$$h(x) = \sum_{j = 0}^{d}\theta_jx_j$$
Parameters AND (inputs) features can be thought of two independent vectors of same size (with x_0 = 1) and $x^i$ is the i'th example.
$$\theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_d \\ \end{bmatrix} \space x^i = \begin{bmatrix} x_{0}^{i} \\ x_{1}^{i} \\ \vdots \\ x_{d}^{i} \\ \end{bmatrix}$$Then there's $y^i$ which is the output of $h(x^i)$ for a specific data input. It's the price of the house in this case. Learning algorithm is picking the parameters $\theta$ to solve your problem
So then what you're trying to do is picking $\theta$ to such that $h_{\theta}(x) \approx y$. 

### Least Squares Method = Linear Regression Method
$$J(\theta) = \frac{1}{2} \sum_{i=1}^n(h_{\theta}(x^i) - y^i)^2$$
	edit: corrected i to start from 0, not 1? ok so I think it's i = 1, because in this case this is talking about the number of data training points you have, not the number of features. number of features is $d$, the number of training points is $n$

Which tells you how far the actual training point is from your prediction. The difference is your "loss". Your goal is to minimize this (find the least). There's a lot of other choices that you could use, but this is the first we are trying for this model. The 1/2 is for derivative you'll be taking later.
#### Gradient Descent - finding the thetas
It's more like an algorithm than a formula.
You first pick a random spot (which does matter) then take a step in the direction of the derivative. Its just an initial condition.
$$\theta^0 := 0$$
$$\theta_{j}^{t+1} := \theta_{j}^t - \alpha\frac{d}{d\theta_j} J(\theta^{t+1})$$
so this is like for each parameter, you add it to the the parameter from the previous step, times the $\alpha$ step size/learning rate, times the partial derivative of the objective function. 
You could use any objective function (any? at least a lot) inside of this algorithm. For the one before, the least squares, we get the partial derivative to be (chain rule)

$$\frac{dJ}{d\theta_j}(\theta) = \sum_{i=1}^{n}\frac{1}{2}\frac{d}{d\theta_j}(h_\theta(x^i) - y^i)^2 = \sum_{i=1}^{n}(h_\theta(x^i)-y^i)\frac{d}{d\theta_j}h_\theta(x^i)$$
then once you take the partial derivative of the unfurled $h_\theta(x^i)$ just gets you 
$$\frac{d}{d\theta_j}h_\theta(x^i) = x^{i}_{j}$$

And you can get your concrete learning rule of 
$$\theta^{t+1}_j := \theta^{t}_{j} - \alpha\sum^{n}_{i=1}(h_\theta(x^i)-y^i)x^i_j$$
then removing the subscripts to simplify you get (into vector notation)
$$\theta^{t+1} := \theta^{t} - \alpha\sum^{n}_{i=1}(h_\theta(x^i)-y^i)x^i$$
	This the GRADIENT. It is the derivative of the COST FUNCTION. 

	Adding features is really heavy, you would have to redo all your training to fit the new feature. Batch needs a different $\alpha$ than minibatch, they need tuning.



Then **Batch** Gradient Descent is when you use all n, but you could also do minibatch gradient descent on $b<n$ points.

#### Normal Equations
$$ \theta = (X^TX)^{-1}X^Ty$$
θ → The parameters that minimize the loss function X → The input feature values for each instance y → The vector of output values for each instance
$$X^TX\theta=X^Ty$$

## Probabilistic Interpretation of Linear Regression
Picking up from our previous definition of Least Squares, we now add noise to $y$, so they are now imperfect and there is some error $\epsilon$ like measurement noise, unmodeled effects, random measurements.
Assume $$y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}$$
#### Properties of $\epsilon$, we want (iid Gaussian)
This is a modeling assumption
1. $\mathbb{E}[\epsilon^{(i)}] = 0$ means the errors are unbiased random variables (on average, its zero)
2. $\mathbb{E}[\epsilon^{(i)}\epsilon^{(j)}] = \mathbb{E}[\epsilon^{(i)}]\mathbb{E}[\epsilon^{(j)}]$ which means the errors are independent
How noisy is the error? You want to have some variance, and we have chose $\sigma^2$ here. 
$\mathbb{E}[(\epsilon^{(i)})^2)] = \sigma^2$ 

$$\epsilon^{(i)} \sim N(0, \sigma^2)$$$$p(\epsilon^{(i)}) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{\epsilon^{(i)}}{2\sigma^2}\right) $$ where (two forms of the gaussian)
- $\epsilon^{(i)}$ represents the variable of interest
- $\mu$ is the mean or expectation of the distribution (also the median and mode) - removed because the mean should be zero.
- $\sigma$ is the standard deviation
- $\sigma^2$ is the variance.

Substituting (now $\theta^Tx$ is the mean, with some variance)
$$y^{(i)} | x^{(i)}_j\theta \sim N(\theta^Tx, \sigma^2)$$
therefore $$ P(y^{(i)} | x^{(i)}_j\theta) = \frac{1}{\sigma\sqrt{2\sigma}}exp(-\frac{y^{(i)} - \theta^Tx^i)^2}{\sigma^2})$$
>> kind of unclear why the theta picks the distribution? Doesn't the error pick the distribution?
>> Ans: yes, the error picks the distribution, but now we are modeling y as a random variable influenced by x and $\theta$, which includes the error. The variance $\sigma^2$ represents the spread of residuals (errors) around the regression line, but is already included in the probability model for $y^{(i)}$. The $\theta$ picked changes the regression line, which is the changing mean. 

Picking the $\theta$ picks the distribution. 
Now you have Likelihood of many distributions, pick "most likely". How likely is the $y$ assuming this distribution and the x's we observe?
$$\begin{align} \mathcal{L}(\theta) & = p(y | x_j\theta) \\ & = ^{n}p(y^{(i)} | x^{(i)}_j\theta) \\
& =  \prod_{i=1}^{n}\frac{1}{\sigma\sqrt{2\sigma}}exp(-\frac{y^{(i)} - \theta^Tx^i)^2}{\sigma^2})\end{align}$$
Since x, y are vectors but independent between $x^{(i)}$ and $y^{(i)}$, you get the second equation.
#### Max Log Likelihood
We are now going to try log likelihood. 
$$\begin{align}
\mathcal{l}(\theta) & = \log \mathcal{L}(\theta) \\
& = \sum_{i=1}^{n} \log\left(\frac{1}{\sigma \sqrt{2\pi}}\right) - \frac{1}{{2\sigma^2}} (y^{(i)} - \theta^T x^{(i)})^2
\end{align}$$
The right most term is what we are about, and we're back around to least squares.
You treat sigma as given
## Relates to:
[[2. Logistic Regression]]: 
1. **Model Structure**: Both models use a linear equation to describe the relationship between the independent variables and the (transformed) dependent variable. In linear regression, the outcome itself is modeled directly; in logistic regression, the log-odds of the outcome is modeled.
2. Linear regression assumes the errors are normally distributed, whereas logistic regression does not make specific assumptions about the distribution of error terms, since it models probabilities.

### Minimum Norm

## Homework
Problem 2 - Linear Regression
(a)
Replacing the $x$ in the general form with $\hat{x}$, the transformed input of the data set gives you the following objective function.
$$J(\theta) = \frac{1}{2} \sum_{i=1}^n(h_{\theta}(\hat{x}^{(i)} - y^{(i)})^2$$
So then taking the partial derivative of $J(\theta)$ and replacing $h_\theta(x) = \theta^T\hat{x}$ as given earlier in the problem, and applying chain rule gives us:
$$\nabla_{\theta} J(\theta) = \sum_{i=1}^{n}(\theta^T\hat{x}-y^{(i)})\frac{d}{d\theta_j}(\theta^T\hat{x}-y^{(i)}) = \sum_{i=1}^{n}(\theta^T\hat{x}^{(i)}-y^{(i)})\hat{x}^{(i)}$$

which makes the gradient descent update rule
$$\theta := \theta - \lambda\sum_{i=1}^n(\theta^T\hat{x}^{(i)}-y^{(i)})\hat{x}^{(i)}$$

(b) Coding
