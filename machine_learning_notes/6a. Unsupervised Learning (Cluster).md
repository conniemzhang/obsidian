#machine_learning #notes 
There's two camps of machine learning, one that thinks everything should be probabilistically modeled, and another that is more interested in nice estimates.

| Structure  | Probability     | Non Probability |
| ---------- | --------------- | --------------- |
| "Cluster"  | GMM             | K-Means         |
| "Subspace" | Factor Analysis | PCA             |


# K Means Clustering


# Mixture of Gaussians

# EM Algorithm (Expectation Maximum)
Used in scenarios when:
1. Incomplete Data: The data set has missing or hidden variables.
2. Mixture Models: The model involves mixtures of different models.
3. Latent Variables: There are variables you cannot observe that affect the data.

Mirrors k-means. Iteratively estimates the parameters of statistical models. 
1. E Step "Guess the latent values" of $z^{(i)}$
	- Calculate the expected values of the log-likelihood function, wrt the current estimate o the distribution of the latent variables. 
1. M step - Update parameters
	- by maximizing the log-likelihood
	
#### E Step
Given Data and Current Values ($\theta, \mu, \sigma....$)
do: Predict $z^{(i)}$ for i = 1...n
Our goal is:
$$
\begin{aligned}
\omega^{(i)}_j &= P(z^{(i)} = j \mid x^{(i)}; \phi, \mu, \sigma) \\
               &= \frac{P(z^{(i)} = j, x^{(i)}; \phi, \mu, \sigma)}{P(x^{(i)}; \phi, \mu, \sigma)} \\
               &= \frac{P(x^{(i)} \mid z^{(i)} = j; \mu, \sigma) P(z^{(i)} = j; \phi)}{\sum_{l=1}^k P(x^{(i)} \mid z^{(i)} = l; \mu, \sigma) P(z^{(i)} = l; \phi)}
\end{aligned}
$$

Calculate the weights given the data point, parameters (likelihoods, mean, variance). If it's really far away probability should be close to zero. 
> This means you can compute reach of these. 

#### M-Step
Given all the $w^{(i)}_j$ our current estimate $P(z^{(i)} = j)$ for i = 1, ... and j = 1....
Do: Estimate observe parameters (use MLE) $\mu, \sigma, phi$. 

eg: $\theta_j = \frac{1}{n}\sum^N_{i=1}w^{(i)}_j$ is approx equal to fraction of all points from source j.

### EM as MLE 
$$\ell(\theta) = \sum\limits_{i = 1}^n {\log P(x^{(i)}; \theta)}$$
$x^{(i)}$ is the data
$\theta$ are the parameters
$$P(x;\theta) = \sum \limits_zP(x, z; \theta)$$
for the latent variables.


![[ml2024_em_mle_img.png]]
Make an easier loss function and iteratively move up the actual loss function.
(E step) Given $\theta^{(t)}$ find $L_t$
(M step) Given $L_t$, set $\theta^{(t+1)} = argmax_\theta L_t(\theta)$

How do we construct $L_t$? For a single term
![[ml2024_em_mle_jensens.png]]
$$\geq \mathop{\mathbb{E}}_{z\sim Q}\left[ log \frac{P(x,z ; \theta)}{Q(z)}\right] \text{Jensen's, log is concave}$$
$$ = \sum_z Q(z)\log\frac{P(x,z ; \theta)}{Q(z)}$$
**Key**: Holds for any Q satisfying (\*) (see image)
This gives us a family of lower bounds $L_t(\theta) \leq l(\theta)$

How do we make it tight?
If the items in Jensen's inequality are constant independent of z, then they would be equal instead of an inequality. Previously we chose any Q. Now the Q has to be related to the $P(x, z;\theta)$.
$$\log\frac{P(x,z;\theta)}{Q(z)} = c$$
$$Q(z) = {P(z | x; \theta)} \text{ because } P(x,z;\theta) = P(z|x; \theta)P(x;\theta)$$
$$c = \log P(x;\theta)$$
N.B.: Q(z) does depend on $\theta$, $x$ -> $Q^{(i)}(z)$ for each i
All Q's are different, they are picked to satisfy the above equation.
We defined the Evidence-Based Lower Bound (ELBO)
$$\text{ELBO}(x, Q, z) = \sum_z Q(z) \log\frac{P(x, z;\theta)}{Q(z)}$$
$$\ell(\theta) \geq \sum^n_{i=1}\text{ELBO}(x^{(i)}, Q^{(i)}; \theta) \text{ for any } Q^{(i)} \text{ satisfies (*)}$$
$$\ell(\theta^{(t)}) = \sum^n_{i=1}\text{ELBO}(x^{(i)}, Q^{(i)}; \theta^{(t)}) \text{ for choice of } Q^{(i)} \text{ above.}$$
**Summary**
![[ml2024_em_mle_summary.png]]
### EM for Gaussian Mixture Models
EM algorithm used to estimate parameters (means, variances, and mixing coefficients).
1. Initialization: Choose initial values of means, variances, and mixing coefficients.
2. E-step: For each data point, calculate the probability that it belongs to each Gaussian component (based on current parameter estimates).
3. M-step: Update the parameters to maximize the likelihood of the data given the probabilities.


