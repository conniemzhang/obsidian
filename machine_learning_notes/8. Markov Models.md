#machine_learning  #rl #notes 

## Quick Overview

![[Pasted image 20241201101704.png]]

Markov Process: 
Markov 
Markov Decision Process: 
Bellman Operator: 
Contraction:
Model:
Q-value:
Policy: 

## Markov Processes - Introduction
Information state: sufficient statistic of history - the result is the same as if you were to have the entire history. Usually used to downsample to a smaller source (all of time vs. 2 hours)
* State $s_t$ is Markov if and only if
$$ p(s_{t+1}|s_t, a_t) = p(s_{t+1}|h_t, a_t) $$
* The future is independent of past given present.
* Simple and often can be satisfied if includes som histry
* Often you can just use the most recent observation
* State representation has big implications
	* Computational complexity.
	* Data required.
	* Resulting performance.




* Transition/dynamics model predicts the next agent state
$$ p(s_{t+1} = s'|s_t = s, a_t = a) $$
Might not proceed to the next state 100% of the time. This represents the distribution of outcomes from a decision.
* Reward model predicts immediate reward
$$ r(s_t = s, a_t = a) = \mathbb{E}[r_t | s_t = s, a_t = a]$$
	- There are several conventions for reward models
		- r(s)  - only dependent on current state
		- r(s, a) - state and action
		- r(s, a, s') - state, action, and future state.


Policy
- Policy $\pi$ determines how the agent chooses actions
- $\pi : S \rightarrow A$ mapping from states to actions
- Deterministic policy:
$$ \pi(s) = a $$
* Stochastic policy
$$\pi(a|s) = Pr(a_t = a | s_t = s) $$
So you have the
Evaluation: estimate or predict the expected reqwards fromt he following a given policy
Control: optimizaation to find the best policy. Takes trial and error and you want to do that in a strategic way.
General procedure for Making Sequences fo Good Decisions Given a Model of the World.
- Assume finite set of states and actions
- Given models fo the world (dynamics and reward)
- Evaluate the performance of a particular decision policy.
- Compute the best policy.
- This can be viewed as a AI planning problem.

## Types if Markov Processes 
Markov Process or Markov Chain
* Memoryless
* No reward function


### Markov Reward Process (MRP)
* can express
* horizon and in episodes

### Markov Decision Processes (MDP)

####  Bellman Equation for MRP
* for a finite state MRP, we can express V(s) using a matrix equation.

Here’s the expanded **matrix form** of the Bellman equation with all terms written explicitly, using $s$ for states and assuming there are $n$ states in total:

$$
\begin{bmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{bmatrix}
=
\begin{bmatrix}
R(s_1) \\
R(s_2) \\
\vdots \\
R(s_n)
\end{bmatrix}
+
\gamma
\begin{bmatrix}
P(s_1, s_1) & P(s_1, s_2) & \cdots & P(s_1, s_n) \\
P(s_2, s_1) & P(s_2, s_2) & \cdots & P(s_2, s_n) \\
\vdots & \vdots & \ddots & \vdots \\
P(s_n, s_1) & P(s_n, s_2) & \cdots & P(s_n, s_n)
\end{bmatrix}
\begin{bmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{bmatrix}.
$$

$$
\mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V}
$$

Where:
- $\mathbf{V}$ is the vector of state values $(V(s)$ for all states $s$).
- $\mathbf{R}$ is the vector of expected immediate rewards $(R(s)$ for all states $s$).
- $\mathbf{P}$ is the state transition probability matrix, where $\mathbf{P}[s, s'] = P(s' \mid s, a)$.
- $\gamma$ is the discount factor.

The presence of $V$ on both sides of the equation highlights that the **Bellman equation** is a **recursive definition** of $V$. When calculating $V$, this typically involves solving the equation numerically or algebraically, depending on the context.

##### Solving for $\mathbf{V}$:
To compute $\mathbf{V}$, you isolate it algebraically:

1. Subtract $\gamma \mathbf{P} \mathbf{V}$ from both sides:
   $$
   \mathbf{V} - \gamma \mathbf{P} \mathbf{V} = \mathbf{R}.
   $$

2. Factor out $\mathbf{V}$ on the left-hand side:
   $$
   (\mathbf{I} - \gamma \mathbf{P}) \mathbf{V} = \mathbf{R},
   $$
   where $\mathbf{I}$ is the identity matrix.

3. Solve for $\mathbf{V}$ using matrix inversion:
   $$
   \mathbf{V} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}.
   $$

##### Why is $V$ on Both Sides Initially?
The recursive nature of the Bellman equation means it’s not directly a simple explicit formula. Instead:
- The left-hand side represents the value function we want to compute.
- The right-hand side defines how that value function relates to itself, immediate rewards, and discounted future rewards.

By isolating $\mathbf{V}$ as shown, you turn the recursive definition into an explicit form for calculation.

##### Numerical Solutions:
In practice, solving $(\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}$ directly may be computationally expensive for large state spaces. Instead:
- Iterative methods like **value iteration** or **policy iteration** approximate $V$ step by step until convergence. These methods exploit the recursive nature of the Bellman equation.
- Solving directly requires taking a matrix inverse ~O(N^3), and requires that (1-gammaP) is invertible.

## Iterative Algorithm for Compting Value of a MRP

## Policy Iteration for MDP
**What:** One option to find the optimal policy for an MDP. It computes the infinite horizon value of the current policy (Policy Evaluation) and improves by updating the policy which is guaranteed to monotonically improve (Policy Improvement).

- Set i = 0
- Initialize $\pi_0(s)$ randomly for all states $s$
- While $i == 0$ or $||\pi_i - \pi_{i-1}||_1 > 0$ ) (L1-Norm, measures if the policy has changed for any state)
	- $V^{\pi_i} \leftarrow \text{MDP V}$ function policy evaluation of \pi_i$
	- $\pi_{i+1} \leftarrow$ Policy improvement
	- $i = i + 1$
- To do this, we have a State-Action value of a policy (the Q-function)
- This represents the expected cumulative reward when starting in state s, taking action a, and following the current policy $\pi$ and follow it foreer.
- $$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^\pi(s'),$$
- Take action a, then follow the policy $\pi$
- Q function is how you figure out what the policy improvement is?

### Steps for Policy Iteration:

1. **Policy Evaluation**:
   Compute the value function \(V^\pi(s)\) for the current policy \(\pi\). This typically involves solving:
   $$ V^\pi(s) = \sum_{a \in A} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^\pi(s') \right]. $$

2. **Q-Function Computation**:
   Once \(V^\pi(s)\) is known, compute the Q-function for all state-action pairs:
   $$ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^\pi(s')$$

3. **Policy Improvement**:
   Update the policy by choosing the action that maximizes the Q-function:
   $$ \pi'(s) = \arg\max_{a \in A} Q^\pi(s, a)$$

## Value Iteration
- What: Maintain optimal value of starting in a state $s$ if have a finite number of steps $k$ left in the episode. Finite number of decisions.
- Iteration to consider longer and longer time horizons.
- You always have an optimal value, but for the wrong horizon.
- This is where value function of a policy must satisfy the Bellman equation.
- Bellman Backup Operator

### Algorithm
- Set k = 1
- Initialize $V_0(s) = 0$ for all states s
- Loop until convergence: (for ex. ||V_{k+1} - V_k||_\inf)
- Keeps going until old value of state and new value of the state is zero.
-


