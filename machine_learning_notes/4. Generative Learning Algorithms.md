#machine_learning #notes #class
## Introduction: 
### Difference between Generative and Discriminative Models
Your statement has the right idea but needs some clarification to accurately distinguish between generative and discriminative learning algorithms. Here’s a more precise explanation:

#### Generative Learning Algorithms
- **Objective**: Model the joint probability distribution \( p(x, y) \).
- **Approach**: First, model the class-conditional densities \( p(x|y) \) and the class priors \( p(y) \). Then, use Bayes' theorem to derive the posterior probability \( p(y|x) \).
- **Example Algorithms**: Naive Bayes, Gaussian Mixture Models, Gaussian Discriminant Analysis (wow).

#### Discriminative Learning Algorithms
- **Objective**: Model the conditional probability distribution \( p(y|x) \) directly, without explicitly modeling \( p(x) \).
- **Approach**: Learn a direct mapping from the input features \( x \) to the output labels \( y \), optimizing the parameters to maximize the conditional likelihood of the labels given the features.
- **Example Algorithms**: Logistic Regression, Support Vector Machines, Neural Networks, Linear Regression, GLM

So, the correct distinction is:

- **Generative models**: Maximize the likelihood of the joint distribution \( p(x, y) \) by modeling \( p(x|y) \) and \( p(y) \).
- **Discriminative models**: Maximize the conditional likelihood \( p(y|x) \) directly.

tl;dr Generative models aim to understand the underlying distribution of the data and model the distribution of each class. Discriminative models focus on the boundary between different classes, and directly model the boundary between classes.

## Multiclass Classification via Softmax Regression
- Distribution over discrete values up to K {cat, dog, bus, car} k = 4
- Encoded as a one-hot vector $y\in{0,1}^k$ boolean vectors of length k
	- ex: k = 4, cat = \[1, 0, 0, 0\], bus = \[0, 0, 1, 0\]
	- Only one of them is positive at a time.
- Multiclass - need to find hyperplanes that separate each class from the other where $\theta_{1-4}\cdot x = 0$, 
- There are the same number of $\theta$ as there are $k$ classes. There are $n$ features, so you will have $k$ parameter vectors, each with $n$ elements.  for $i = 1, 2, 3...k$
$$θ_i​=[θ_{i1}​,θ_{i2​},θ_{i3}​,…,θ_{in}​]$$
* For each class $i$ you compute the dot product of its parameter vector $\theta_i$ with the feature vector $X_{\{k\}}$. 


- Turn a set of outcomes into a probability distribution by raising to the $e$
This gives you the softmax equation, which you could prove is in the exponential family.
$$p(y=k) = \frac{exp(\theta_k\cdot x)}{\sum_{y=1..k}exp(\theta\cdot x)}$$
You minimize the Cross-Entropy (which looks a lot like the log of the softmax equation)
$$(p_1, \hat{p}) = -\sum\limits_{y=1}^{k}p(y)log(\hat{p}(y))$$
Cross Entropy intuitively is a MLE for the softmax equation. It's maximizing the distance to the ground truth. 

## Generative Learning Algorithm s
Gaussian discriminant analysis GDA)
Naive Bayes
The ones we have been learning in the past is discriminative learning algotihms
- model/parameterize p(y|x)
- General form y | x ; \theta = Exponential Distribution(\eta) \eta = \theta x

NOW we are going to model/parameterize for the joint distribution of $p(x,y) = p(x|y)p(y)$
with again that $x$ is a label, $y$ is the class/label, $p(y)$ is the prior for the class/label.
so you are learning the distributions $p(x|y), p(y)$
Test time - compute $p(y | x)$ by Bayes rule. Bottom is marginal probability.
$$p(y|x) = \frac{p(x|y)p(y)}{p(x)} = \frac{px|y)p(p)}{\sum_{y'}p(x|y')p(y')}$$
>> ?? Did we have probabilities of the result from the softmax version?

## Gaussian Discriminant Analysis
* Continuous x, y (the labels) is discrete
* Method focuses on finding a function that "discriminates" the boundary between multiple classes, assuming all classes are normally distributed.
Suppose $x \in \mathbb{R}^d$, and assume $p(x|y)$ is Gaussian distribution $x|y \sim N(x, y)$, and x is a high dimensional vector. Example: cancer classification, benign vs cancerous. 
GDA definition:
$$\begin{align*} & y \in \{0, 1\} \\ & x \mid (y = 0) \sim \mathcal{N}(\mu_0, \xi^2) \\ & x \mid (y = 1) \sim \mathcal{N}(\mu_1, \xi^2) \\ & p(y = 1) = \phi \\ & p(y = 0) = 1 - \phi \end{align*}$$

Notes:
- Both averages are different bc both variables have different Gaussian distributions.
- Called gaussian bc it uses gaussian distribution, but what is the discriminant. 
- y is Bernoulli because it's one of two classes.
- parameters are $\mu_0, \mu_1, \xi, \phi$

#### How do we learn the parameters?
Fit parameters through the principle of Maximum Likelihood.
What is Likelihood? $$\mathcal{L}(\phi, \mu_0, \mu_1, \xi) = p((x^{1}, y^{1})...(x^n, y^n);\phi,\mu_0,\mu_1,\xi)$$
$(x^i, y^i)$ is the i'th example. Lets assume all likelihoods are joined independently. You can then factorize and use chain rule: 
$$\begin{align*}
\mathcal{L}(\phi, \mu_0, \mu_1, \xi) &= \prod_{i=1}^{n}p((x^{(i)}, y^{(i)});\phi,\mu_0,\mu_1,\xi) \\
&= \prod_{i=1}^{n}p((x^{(i)}, y^{(i)});\mu_0,\mu_1,\xi)p(y^{(i)};\phi)\end{align*}$$
Compare the above to the form for discriminative below, where we do not model the distributions of x and y. Notice we only have parameter $\theta$, which is all we know.
$$p(y^{(i)}..y^{(i)} | x^{(i)}...x^{(n)};\theta) = \prod_{i=1}^{n}p(y^{(i)}|x^{(i)};\theta)$$
##### Computing the likelihood by hand: $\mathcal{L}(\phi,\mu_0,\mu_1,\xi)$
$$\begin{align}
\text{argmax } \mathcal{L}(\phi, \mu_0, \mu_1, \xi) &= \text{argmax } \log(\mathcal{L}(\phi, \mu_0, \mu_1, \xi)) \\
&= \text{argmax } \ell(\phi, \mu_0, \mu_1, \xi) \\
&= \sum_{i=1}^{n} \log p(x^{(i)} | y^{(i)} ; \mu_0, \mu_1, \xi) + \sum_{i=1}^{n}\log p(y^{(i)};\phi)
\end{align}$$
Note log likelihood $\ell$ that we've seen earlier.
How do you maximize this function? From calculus: if $\theta$ is a maximizer of $f(\theta)$, then $\nabla f(\theta) = 0$.
Solve the equation:
$$\ell(\phi, \mu_0, \mu_1, \xi) = 0$$
$$\frac{\partial\ell}{\partial\phi}=0, \frac{\partial\ell}{\partial\mu_0}=0, \frac{\partial\ell}{\partial\mu_1}=0, \frac{\partial\ell}{\partial\xi}=0$$
Solutions of MLE:
$$\mathcal{U}_1 = \{i; y^{(i)} = 1\}$$
$$\mathcal{U}_0 = \{i; y^{(i)} = 0\}$$
Set of indices of positive examples, and set of negative examples. $n$ is total number of examples, and the sum of the size of the sets.
$$\phi = \frac{|\mathcal{U_1}|}{n}, (1-\phi) = \frac{|\mathcal{U}_0|}{n}$$
Now we have an indicator function $\mathbb{1}(E)$ = 1 if E happens, 0 otherwise. So $\phi$ is defined as:
$$\phi = \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}(y^{(i)} = 1)$$
Now looking at the MLE of $\mu$
$$\mu_0 = \frac{1}{|\mathcal{U}_0|} * \sum_{i\in\mathcal{U_0}}x^{(i)}$$
which is the average of all the negative x's.
$$\begin{align*}
\sum &= \frac{1}{n} \sum_{i=1}^{n} (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T 
\\
&= \frac{1}{n}\left(\sum_{i \in \mathcal{U}_0} (x^{(i)} - \mu_{0})(x^{(i)} - \mu_{0})^T + \sum_{i \in \mathcal{U}_1} (x^{(i)} - \mu_1)(x^{(i)} - \mu_{1})^T\right)
\end{align*}$$
 
 and it seems to be the variance of the examples.

##### Prediction with GDA
Given {x}, output $y\in{0,1}$.
Output the $\text{argmax } p(y | x;\phi, \mu_0, \mu_1,\xi)$ from the solutions of the MLE above. You get the two scalars, for positive or negative and you want the one that is bigger.
Bayes rule (kind of glossed over in lecture)
$$p(y=1|x;\phi, \mu_0,\mu_1,\xi) = \frac{p(x|y=1;\mu_1,\xi)p(y=1;\phi)}{p(x;\phi,\mu_0,\mu_1,\xi)} = \frac{1}{1 + \exp(-\theta^Tx+\theta_0)}$$
$\theta\in\mathbb{R}^d$, $\theta_0\in\mathbb{R}$ depend on $\phi, \mu_0, \mu_1,\xi$

Then you get a decision boundary which is a linear function of x  $\theta^Tx + \theta_0 \geq 0$ for this case. This derivation was in lecture but not written down but you get the linear function.
$$\frac{1}{1 + \exp(-\theta^Tx+\theta_0)} = 0.5$$
### Naive Bayes Spam Classification Filter
Naive Bayes: Assume $x_1, ..., x_d$ are independent conditioned on y.
