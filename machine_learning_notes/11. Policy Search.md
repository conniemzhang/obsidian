#reinforcement_learning #notes
# Policy Gradient Methods
- direct parametrization of the policy
Score Functions and Policy Gradients
Softmax Policy: weight actions using linear combination of features $\phi(s,a)^T\theta%

Gaussian Policy

- Doesn't require the reward fun/ction to be differentiable, only the policy parametrization
- Doesn't need to be Markov
- Must be finite horizon so m > 1, episodic.

## Monte Carlo Policy Gradient
$$ \nabla_\theta V(\theta) = (1/m) \sum_{i=1}^m R(\tau^{(i)})\sum_{t=0}^{\tau -1} \nabla_\theta \log\pi_\theta(a_t^{(i)} | s_t^{(i)}) $$
- m is the number of sampled trajectories (episodes), which ensures the gradient is unbiased.
- $\tau^{(i)}$ is the i-th trajectory sampled under the policy.
- $R(\tau^{(i)})$ is the total reward of the i-th trajectory.
- This discrete summation approximates the expectation in the common policy gradient formula. Monte Carlo method.
- Separates the summations:
	- Outer summation: Over sampled trajectories $i = 1, \dots, m$.
	- Inner summation: Over time steps $t = 0, \dots, \tau - 1$ in each trajectory.
- The return $G_t^i$  (cumulative discounted reward) is computed as the actual sum of rewards observed from a trajectory.
$$G_t^i = \sum_{k=1}^T \gamma^{k-t}r_k$$
- where $\gamma$ is the discount factor.
- Key Problems:
	- High variance - actual trajectory returns $G_t^i$ can vary a lot between episodes
	- Delayed updates - Monte Carlo methods must wait until the end of the episode to calculate $G_t^i$ 

## REINFORCE - Policy Gradients with Temporal Structure
- Monte Carlo are unbiased but very noisy
- Fixes that can make them practical
	- Temporal Structure
	- Baseline
- We can repeat the same argument sum over all time steps


Likelihood ratio / score function policy gradient
* Baseline
* Alternative Targets
Advanced Policy Gradient Methods
- Proximal policy Optimization (PPO)

### Baseline
- What: reduce variance by introducing a baseline b(s). It is a reference point to see how good the total return $R_t$ is compared to what is expected.
$$ \nabla_\theta \mathbb{E}_T[R] = \mathbb{E}_T \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t | s_t; \theta) \left( \sum_{t'=t}^{T-1} - b(s) \right)\right] $$
- For any choice of b, the gradient estimator is unbiased.
- Near optimal choice is the expected return is below, which is the expected return starting from state $s_t$
$$ b(s_t) \approx \mathbb{E}[r_t + t_{t+1} + \dots + r_{T-1}] $$
* Interpretation: increase logprob of action $a_t$ proportionally to how much returns $\sum_{t'=t}^{T-1}r_{t'}$ are better than expected.

## Actor-Critic
- Alternatives to using Monte Carlo - actor critic
- Actor-Critic uses a critic, a value function approximator, as an estimate of expected returns.
	- Maintains an explicit representation of policy and the value function, updates both.
	- $\theta$ as the actor
	- $V / Q$ as the critic
	- A3C is a popular actor critic method.
- The critic provides $V^\pi(s_t)$, an estimate of the expected discounted return from state $s_t$​:

$$V^\pi(s_t) \approx \mathbb{E}_\pi \left[ \sum_{k=t}^\infty \gamma^{k-t} r_k \, \bigg| \, s_t \right].$$

N-Step Estimators


## Comparison of Actor-Critic vs Monte Carlo

| **Aspect**                      | **Monte Carlo (e.g., REINFORCE)**                                                                                  | **Actor-Critic**                                                                                            |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| **Objective**                   | Estimates the policy gradient using returns $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ computed from full trajectories. | Combines **policy gradient (actor)** with a value function (critic) to estimate gradients more efficiently. |
| **Estimate of Return**          | Uses the actual observed return $G_t$ (Monte Carlo sampling).                                                      | Uses the critic’s value function $V(s_t)$, sometimes combined with $r_t + \gamma V(s_{t+1})$.               |
| **Update Frequency**            | Updates after the entire trajectory is completed (episodic).                                                       | Updates at every time step (online learning).                                                               |
| **Variance**                    | High variance because $G_t$ varies across episodes.                                                                | Lower variance since the critic’s $V(s_t)$ smooths out return estimates.                                    |
| **Bias**                        | Unbiased because it uses the actual trajectory returns.                                                            | May introduce bias if the critic $V(s_t)$ is not a perfect estimate of the true value function.             |
| **Exploration vs Exploitation** | Can suffer from poor exploration because $G_t$ reflects a single sampled trajectory.                               | Balances exploration and exploitation better with continuous updates.                                       |
| **Sample Efficiency**           | Low sample efficiency: Requires many full episodes to get good gradient estimates.                                 | Higher sample efficiency: Uses partial trajectory data for incremental updates.                             |
| **Learning Stability**          | Often unstable due to high variance in gradient estimates.                                                         | More stable learning because of variance reduction from the critic.                                         |
| **Computational Complexity**    | Simple to implement (no value function approximation).                                                             | More complex: Requires learning and updating both the actor (policy) and the critic (value function).       |
| **Discounted Return (γ\gamma)** | Uses the cumulative discounted return directly.                                                                    | Uses value estimates combined with immediate rewards $r_t + \gamma V(s_{t+1})$.                             |
| **Episodic vs Continuous**      | Designed for episodic tasks, requiring the full trajectory to compute returns.                                     | Can be used for both episodic and continuous tasks due to incremental updates.                              |
| **Primary Use Case**            | Useful for problems where trajectories are short and environment noise is low.                                     | Preferred for longer tasks or stochastic environments with high variability.                                |

---

### Is Monte Carlo Still Appropriate?

Monte Carlo methods, like REINFORCE, are not **obsolete** but have specific use cases where they may still be a good choice:

#### When Monte Carlo is Appropriate:

1. **Short Trajectories:**
    - For problems with short episodes (e.g., games with quick resets or simple tasks), Monte Carlo methods are computationally efficient because full returns are easy to compute.
2. **Low Environment Noise:**
    - When the environment is deterministic or nearly deterministic, the variance in $G_t$ is small, making Monte Carlo methods work well without additional variance reduction.
3. **Simpler Implementation:**
    - Monte Carlo methods are simpler to implement since they don’t require maintaining or learning a value function (as in actor-critic).
4. **Unbiased Learning:**
    - Monte Carlo methods produce unbiased gradient estimates because they use the actual observed returns rather than estimates.

---

#### Why Actor-Critic is Often Preferred:

1. **Better Scalability:**
    - Actor-critic methods are more sample-efficient, particularly in environments with long episodes or continuous tasks.
2. **Variance Reduction:**
    - Actor-critic methods significantly reduce variance by using the critic’s value function, which leads to more stable and faster learning.
3. **Real-Time Updates:**
    - Actor-critic allows updates at every time step, making it suitable for continuous environments where episodic resets are impractical.

---

### **Summary**

Monte Carlo methods like REINFORCE are **not defunct** but are less commonly used in complex tasks where actor-critic methods excel due to their efficiency and stability. However, Monte Carlo remains a good choice for **simple, low-variance problems** with short trajectories, where its unbiased nature and simplicity make it advantageous. Actor-critic methods, on the other hand, are better suited for modern RL applications involving long horizons, high noise, or continuous action spaces.