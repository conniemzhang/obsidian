#machine_learning #notes 

 # Bias Variance
**bias**: lack of expressivity, doesn't depend much on the data. 
> note, for linear models it doesn't depend at all, for nonlinear there is the much.

**variance**: lack of data or too complex of a model
test error $L(\theta)$ = bias^2 + variance
![[ml2024_dbl_descent_1.png]]
These can't be directly evaluated. In practice you try several different models and select it based off a validation set. This graph just directs you in your model selection. 

# Double Descent (7.2)
This bias-variance model was found to miss a phenomenon, referred to as Double Descent.
https://mlu-explain.github.io/double-descent/
![[ml2024_ddl_descent_2.png]]
To the right of the interpolation threshold, the behavior changes. The over-parameterized region is called the interpolation regime. Number of parameters > number of data points, in many cases you'll see a second descent.

The peak, when the x-axis is the number of data points, peaks when n, number of data is approximately equal to d, the number of parameters.

Intuition/explainations for linear models
- Existing algorithms: underperforms dramatically when n is close to d.
	- If you chance algorithms, you wouldn't see the peak?
	- Existing algorithm being like gradient descent
	- The norms of the $\theta$ is big when $n \approx d$ so they think it leads to the peak.
- When # params >> n
	- There is implicit regularization effect which makes the norms small.
What is the measure of complexity?
* Norm is one measure, but does not have a strict answer.
Regularization (which tries to reduce the norms), can mitigate the peak.





